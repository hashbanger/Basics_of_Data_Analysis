{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term Frequency - Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook Created by [Prashant Brahmbhatt](https://github.com/hashbanger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Valueable references:  \n",
    "\n",
    "[Christian S. Perone](https://github.com/perone)'s Blog  \n",
    "[Wikipedia::tf-idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Tf-Idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF is used to figure out how important is a word in a document. tf-idf are is a very interesting way to convert the textual representation of information into a **Vector Space Model (VSM)**, or into sparse features.  \n",
    "VSM is an algebraic model representing textual information as a vector, the components of this vector could represent the importance of a term (tf–idf) or even the absence or presence (Bag of Words) of it in a document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in modeling the document into a vector space is to create a dictionary of terms present in documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to select all terms from the document and convert it to a dimension in the vector space, but we would want to remove the **stopwords** first that are present in almost all documents. We want to extract important features from documents, features that could identify them among other similar documents. So using terms like “the, is, at, on”, etc.. is unhelpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lets us create a toy case for as our data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = (\"The sky is blue.\",                          #d1\n",
    "             \"The sun is bright.\")                        #d2\n",
    "test_set = (\"The sun in the sky is bright.\",              #d3\n",
    "            \"We can see the shining sun, the bright sun.\")#d4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to create a index vocabulary (dictionary) of the words of the train document set, using the documents **d1** and **d2** from the document set, we’ll have the following index vocabulary denoted as ***E(t)*** where the t is the term:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathrm{E}(t) = \\begin{cases} 1, & \\mbox{if } t\\mbox{ is ``blue''} \\\\ 2, & \\mbox{if } t\\mbox{ is ``bright''} \\\\ 3, & \\mbox{if } t\\mbox{ is ``sky''} \\\\ 4, & \\mbox{if } t\\mbox{ is ``sun''} \\\\ \\end{cases}$$\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’re going to use the term-frequency to represent each term in our vector space.  \n",
    "The term-frequency is nothing more than a measure of how many times the terms present in our vocabulary ***E(t)*** are present in the documents **d3** or **d4**, we define the term-frequency as a couting function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathrm{tf}(t,d) = \\sum\\limits_{x\\in d} \\mathrm{fr}(x, t) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where the ***fr(x, t)*** is a simple function defined as:  \n",
    "$$\\mathrm{fr}(x,t) = \\begin{cases} 1, & \\mbox{if } x = t \\\\ 0, & \\mbox{otherwise} \\\\ \\end{cases} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***tf(t, d)*** returns how many times the term **t** is present in the document **d**  \n",
    "example: ***tf(\"sun\", d4)*** = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating document vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding how Tf works, we can move on to the creation of the document vector, which is represented by:  \n",
    "$$  \\displaystyle \\vec{v_{d_n}} =(\\mathrm{tf}(t_1,d_n), \\mathrm{tf}(t_2,d_n), \\mathrm{tf}(t_3,d_n), \\ldots, \\mathrm{tf}(t_n,d_n)) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documents **d3** and **d4** can be represented in vectors as:  \n",
    "    $$ \\vec{v_{d_3}} = (\\mathrm{tf}(t_1,d_3), \\mathrm{tf}(t_2,d_3), \\mathrm{tf}(t_3,d_3), \\ldots, \\mathrm{tf}(t_n,d_3)) \\\\ \\vec{v_{d_4}} = (\\mathrm{tf}(t_1,d_4), \\mathrm{tf}(t_2,d_4), \\mathrm{tf}(t_3,d_4), \\ldots, \\mathrm{tf}(t_n,d_4)) $$  \n",
    "    which evaluates to:  \n",
    "    $$ \\vec{v_{d_3}} = (0, 1, 1, 1) \\\\ \\vec{v_{d_4}} = (0, 1, 0, 2) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here in d4 there is no occurence of the words **blue** and **sky** hence the 0 value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a collection of documents, now represented by vectors, we can represent them as a matrix with **D x F** shape, where **|D|** is the cardinality of the document space, or how many documents we have and the F is the number of features, in our case represented by the vocabulary size.  \n",
    "$$ M_{|D| \\times F} = \\begin{bmatrix} 0 & 1 & 1 & 1\\\\ 0 & 2 & 1 & 0 \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **sklearn**, what we have presented as the term-frequency, is called **CountVectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words= 'english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **CountVectorizer** already uses as default **“analyzer”** called **WordNGramAnalyzer**, which is responsible to convert the *text to lowercase, accents removal, token extraction, filter stop words,* etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sun': 3, 'sky': 2, 'blue': 0, 'bright': 1}\n"
     ]
    }
   ],
   "source": [
    "vectorizer.fit_transform(train_set)\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the vocabulary is same as we supposed in ***E(t)*** except here it begins from 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now using the same vectorizer to create sparse matrix for our **test_set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 3)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 3)\t2\n"
     ]
    }
   ],
   "source": [
    "test_matrix = vectorizer.transform(test_set)\n",
    "print(test_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This **test_matrix** is a sparse matrix curretly in **Coordinate Format** but can be coverted into dense format.  \n",
    "Its dimensions will be as discussed **|D| x F**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 1, 1, 1],\n",
       "        [0, 1, 0, 2]], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_matrix.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main problem with the term-frequency approach is that it scales up frequent terms and scales down rare terms which are empirically more informative than the high frequency terms.  \n",
    "The basic intuition is that a term that occurs frequently in many documents is not a good discriminator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf-idf gives is how important is a word to a document in a collection, and that’s why tf-idf incorporates local and global parameters, because it takes in consideration not only the isolated term but also the term within the document collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf-idf scales down the frequent terms and scales up the rare occuring words. It does that using a logarithmic scale.    \n",
    "We can remove stopwords as generally pre defined in the stopwords in library but a better way would be to,  \n",
    "\n",
    "\"convert the entire documents in tf-idf weights and then remove the words with value lower than decided threshold.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going back to our definition of the **tf(t,d)** which is actually the term count of the term t in the document d.  \n",
    "The use of this simple term frequency could lead us to problems like **keyword spamming**, which is when we have a repeated term in a document with the purpose of improving its ranking on an IR (Information Retrieval) system or even create a bias towards long documents, making them look more important than they are just because of the high frequency of the term in the document.  \n",
    "\n",
    "So the term frequency **tf(t,d)** of a document on a vector space is usually also normalized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**----------------------------------------------------------- Optional Maths ----------------------------------------------------------------------------------**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to normalize **d4** - \"We can see the shining sun, the bright sun.\"  \n",
    "It's vector representation was,  \n",
    "$$\\vec{v_{d_4}} = (0, 1, 0, 2) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To normalize the vector, is the same as calculating the *Unit Vector* of the vector, and they are denoted using the **“hat”** notation: **v^ (v hat)**.  \n",
    "The definition of the unit vector **v^** of a vector **v** is:\n",
    "\n",
    "$$\\displaystyle \\hat{v} = \\frac{\\vec{v}}{\\|\\vec{v}\\|_p} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the length of a vector \n",
    "$$\\vec{u} = (u_1, u_2, u_3, \\ldots, u_n)$$\n",
    "is calculated using the Euclidean norm – a norm is a function that assigns a strictly positive length or size to all vectors in a vector space -, which is defined by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\|\\vec{u}\\| = \\sqrt{u^2_1 + u^2_2 + u^2_3 + \\ldots + u^2_n} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sometimes see that a **p** with vector as with **v** in the unit vector formula. Because it can be generalized as -  \n",
    "$$ \\displaystyle \\|\\vec{u}\\|_p = ( \\left|u_1\\right|^p + \\left|u_2\\right|^p + \\left|u_3\\right|^p + \\ldots + \\left|u_n\\right|^p )^\\frac{1}{p} \\\\  \\displaystyle \\|\\vec{u}\\|_p = (\\sum\\limits_{i=1}^{n}\\left|\\vec{u}_i\\right|^p)^\\frac{1}{p} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we read about a **L2-norm**, we’re reading about the Euclidean norm, a norm with **p=2**, the most common norm used to measure the length of a vector, typically called **“magnitude”**; actually, when you have an unqualified length measure (without the p number), you have the L2-norm (Euclidean norm).  \n",
    "\n",
    "When we read about a **L1-norm**, we’re reading about the norm with **p=1**, defined as:  \n",
    "\n",
    "$$\\displaystyle \\|\\vec{u}\\|_1 = ( \\left|u_1\\right| + \\left|u_2\\right| + \\left|u_3\\right| + \\ldots + \\left|u_n\\right|) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is nothing more than a simple sum of the components of the vector, also known as Taxicab distance, also called **Manhattan distance.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\hat{v} = \\frac{\\vec{v}}{\\|\\vec{v}\\|_p} \\\\ \\\\ \\hat{v_{d_4}} = \\frac{\\vec{v_{d_4}}}{||\\vec{v_{d_4}}||_2} \\\\ \\\\ \\\\ \\hat{v_{d_4}} = \\frac{(0,1,0,2)}{\\sqrt{0^2 + 2^2 + 1^2 + 0^2}} \\\\ \\\\ \\hat{v_{d_4}} = \\frac{(0,1,0,2)}{\\sqrt{5}} \\\\ \\\\ \\small \\hat{v_{d_4}} = (0.0, 0.4472136 , 0.0, 0.89442719) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**--------------------------------------------------------------------------------------------------------------------------------------------------------------------**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cardinality of our document space is defined by $$\\left|{D_{train}}\\right| = 2 \\\\ \\left|{D_{test}}\\right| = 2$$  \n",
    "\n",
    "since we have only 2 two documents for training and testing, but they obviously don’t need to have the same cardinality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inverse Document frequency (IDF)**is defined as:  \n",
    "$$ \\displaystyle \\mathrm{idf}(t) = \\log{\\frac{\\left|D\\right|}{1+\\left|\\{d : t \\in d\\}\\right|}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $$\\left|\\{d : t \\in d\\}\\right|$$ is the number of documents where the term t appears, when the term-frequency function satisfies $$ \\mathrm{tf}(t,d) \\neq 0$$we’re only adding 1 into the formula to avoid zero-division."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the **Tf-Idf** becomes \n",
    "$$\\mathrm{tf\\mbox{-}idf}(t) = \\mathrm{tf}(t, d) \\times \\mathrm{idf}(t) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this formula has an important consequence: a high weight of the tf-idf calculation is reached when you have a high term frequency (tf) in the given document (local parameter) and a low document frequency of the term in the whole collection (global parameter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the idf for each feature present in the feature matrix with the term frequency we have calculated in the first tutorial:\n",
    "\n",
    "M_{train} = $$\\begin{bmatrix} 0 & 1 & 1 & 1\\\\ 0 & 2 & 1 & 0 \\end{bmatrix}\n",
    "\n",
    "Since we have 4 features, we have to calculate \n",
    "\n",
    "\n",
    "$$\\mathrm{idf}(t_1) = \\log{\\frac{\\left|D\\right|}{1+\\left|\\{d : t_1 \\in d\\}\\right|}} = \\log{\\frac{2}{1}} = 0.69314718$$\n",
    "\n",
    " \n",
    "\n",
    "$$\\mathrm{idf}(t_2) = \\log{\\frac{\\left|D\\right|}{1+\\left|\\{d : t_2 \\in d\\}\\right|}} = \\log{\\frac{2}{3}} = -0.40546511$$\n",
    "\n",
    " \n",
    "\n",
    "$$\\mathrm{idf}(t_3) = \\log{\\frac{\\left|D\\right|}{1+\\left|\\{d : t_3 \\in d\\}\\right|}} = \\log{\\frac{2}{3}} = -0.40546511$$\n",
    "\n",
    " \n",
    "\n",
    "$$\\mathrm{idf}(t_4) = \\log{\\frac{\\left|D\\right|}{1+\\left|\\{d : t_4 \\in d\\}\\right|}} = \\log{\\frac{2}{2}} = 0.0$$\n",
    "\n",
    "These idf weights can be represented by a vector as:\n",
    "\n",
    "$$\\vec{idf_{train}} = (0.69314718, -0.40546511, -0.40546511, 0.0) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
